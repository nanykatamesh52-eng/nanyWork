import os
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

load_dotenv()  # Load your OpenAI key

# Cache models to avoid rebuilding every request
_vector_db = None
_embedding_model = None
_llm = None


def get_nphies_answer(query: str, language: str = "English"):
    """
    ğŸ”¬ Multilingual Retrieval-Augmented Generation (RAG) pipeline.
    Handles both English and Arabic queries automatically.

    Steps:
        1. Load and index Nphies Q&A file (FAISS + embeddings)
        2. Encode the query (semantic embedding)
        3. Retrieve similar context
        4. Generate answer using OpenAI GPT model in selected language
    """

    global _vector_db, _embedding_model, _llm

    if not query or not query.strip():
        return "âš ï¸ Please enter a question." if language == "English" else "âš ï¸ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø³Ø¤Ø§Ù„."

    file_path = "Nphies Q-A.txt"
    if not os.path.exists(file_path):
        return (
            "âš ï¸ Nphies Q&A file not found."
            if language == "English"
            else "âš ï¸ Ù…Ù„Ù Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Nphies Q-A.txt ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯."
        )

    # 1ï¸âƒ£ Build once (cache)
    if _vector_db is None:
        print("ğŸ“š Loading and indexing Nphies Q&A knowledge base...")

        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()

        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        docs = splitter.split_documents(documents)

        # multilingual embedding model
        _embedding_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )

        _vector_db = FAISS.from_documents(docs, _embedding_model)
        print(f"âœ… Indexed {len(docs)} text chunks.")

        _llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2)

    # 2ï¸âƒ£ Convert query â†’ embedding vector
    print("\nğŸ”¹ Generating embedding for query...")
    query_vector = _embedding_model.embed_query(query)

    # 3ï¸âƒ£ Retrieve top similar text chunks
    print("\nğŸ”¹ Performing FAISS similarity search...")
    results = _vector_db.similarity_search_by_vector(query_vector, k=3)
    if not results:
        return (
            "âŒ No relevant answer found in the Nphies database."
            if language == "English"
            else "âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø´Ø§Ø¨Ù‡Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù†ÙÙŠØ³."
        )

    # Combine retrieved context
    context = "\n\n".join([r.page_content for r in results])

    # 4ï¸âƒ£ Language-aware prompt
    if language == "Arabic":
        system_prompt = (
            "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ù†Ø¸Ø§Ù… Ù†ÙÙŠØ³ (NPHIES). "
            "Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø¯Ù‚Ø© ÙˆØ¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:\n\n"
        )
        user_prompt = f"{system_prompt}{context}\n\nØ§Ù„Ø³Ø¤Ø§Ù„: {query}\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"
    else:
        system_prompt = (
            "You are an intelligent assistant specialized in the NPHIES system. "
            "Use the following context to answer the question clearly and in English:\n\n"
        )
        user_prompt = f"{system_prompt}{context}\n\nQuestion: {query}\nAnswer:"

    # 5ï¸âƒ£ Generate reasoning-based answer
    print("\nğŸ”¹ Sending to GPT reasoning model...")
    response = _llm.invoke(user_prompt)
    return response.content.strip()
